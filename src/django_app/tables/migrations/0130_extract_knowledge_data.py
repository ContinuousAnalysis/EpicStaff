# Generated by Django 5.1.3 on 2026-01-19 13:29
# This migration extracts data from OLD models before they are deleted

import json
import os
from datetime import datetime
from django.db import migrations
from django.conf import settings
from loguru import logger


# Use MEDIA_ROOT (mounted volume) to persist backup across container rebuilds
# Fallback to /app/media if MEDIA_ROOT is not set
MEDIA_ROOT = getattr(settings, "MEDIA_ROOT", None) or "/app/media"
BACKUP_DIR = os.path.join(MEDIA_ROOT, "knowledge_migration_backup")
CONTENTS_DIR = os.path.join(BACKUP_DIR, "contents")


def check_table_exists(connection, table_name):
    """Check if a table exists in the database."""
    with connection.cursor() as cursor:
        cursor.execute(
            """
            SELECT EXISTS (
                SELECT FROM information_schema.tables
                WHERE table_name = %s
            )
            """,
            [table_name],
        )
        return cursor.fetchone()[0]


def extract_knowledge_data(apps, schema_editor):
    """
    Extract knowledge data from OLD models before they are deleted.

    Extracts:
    - SourceCollection: collection_name, user_id, embedder_id, linked agent_ids
    - DocumentMetadata: file_name, file_type, chunk params
    - DocumentContent: binary content (saved as files)
    - Agent search configs: search_limit, similarity_threshold
    """

    connection = schema_editor.connection

    # Check if old tables exist (for new DB case)
    if not check_table_exists(connection, "tables_sourcecollection"):
        logger.info(
            "Old knowledge tables do not exist (new database). Skipping extraction."
        )
        return

    # These represent the model state at this point in migration history
    try:
        SourceCollection = apps.get_model("tables", "SourceCollection")
        DocumentMetadata = apps.get_model("tables", "DocumentMetadata")
        DocumentContent = apps.get_model("tables", "DocumentContent")
        Agent = apps.get_model("tables", "Agent")
    except LookupError as e:
        logger.warning(f"Could not load historical models: {e}. Skipping extraction.")
        return

    # Check if there's any data to extract
    try:
        collections_count = SourceCollection.objects.count()
        if collections_count == 0:
            logger.info("No collections found in database. Skipping extraction.")
            return
        logger.info(f"Found {collections_count} collections to export")
    except Exception as e:
        logger.warning(f"Could not query SourceCollection: {e}. Skipping extraction.")
        return

    # Create backup directories
    os.makedirs(CONTENTS_DIR, exist_ok=True)
    logger.info(f"Created backup directory: {BACKUP_DIR}")

    collections_data = []
    agents_search_config = []
    errors = []

    # Process each collection using ORM
    for col_idx, collection in enumerate(SourceCollection.objects.all()):
        try:
            collection_name = collection.collection_name
            user_id = collection.user_id
            embedder_id = collection.embedder_id  # FK id, not the object

            # Get agents linked to this collection
            try:
                agent_ids = list(
                    Agent.objects.filter(
                        knowledge_collection_id=collection.pk
                    ).values_list("id", flat=True)
                )
            except Exception as e:
                logger.warning(
                    f"Could not fetch agents for collection '{collection_name}': {e}"
                )
                agent_ids = []

            docs_data = []
            try:
                documents = DocumentMetadata.objects.filter(
                    source_collection_id=collection.pk
                )
            except Exception as e:
                logger.warning(
                    f"Could not fetch documents for collection '{collection_name}': {e}"
                )
                documents = []

            for doc_idx, document in enumerate(documents):
                try:
                    file_name = document.file_name
                    file_type = document.file_type
                    chunk_strategy = document.chunk_strategy
                    chunk_size = document.chunk_size
                    chunk_overlap = document.chunk_overlap
                    additional_params = document.additional_params

                    # Handle additional_params - ensure it's a dict
                    if isinstance(additional_params, str):
                        try:
                            additional_params = json.loads(additional_params)
                        except json.JSONDecodeError:
                            additional_params = {}
                    elif additional_params is None:
                        additional_params = {}

                    # Get binary content if exists
                    content_file = None
                    if document.document_content_id:
                        try:
                            content_obj = DocumentContent.objects.filter(
                                pk=document.document_content_id
                            ).first()

                            if content_obj and content_obj.content:
                                # Save binary content to file
                                safe_filename = "".join(
                                    c if c.isalnum() or c in "._-" else "_"
                                    for c in (file_name or "unnamed")
                                )
                                content_file = (
                                    f"col_{col_idx}_doc_{doc_idx}_{safe_filename}.bin"
                                )
                                content_path = os.path.join(CONTENTS_DIR, content_file)

                                with open(content_path, "wb") as f:
                                    # Handle memoryview or bytes
                                    content = content_obj.content
                                    if isinstance(content, memoryview):
                                        content = bytes(content)
                                    f.write(content)

                                logger.info(
                                    f"Saved content for document '{file_name}' to {content_file}"
                                )
                        except Exception as e:
                            logger.error(
                                f"Failed to save content for document '{file_name}': {e}"
                            )
                            errors.append(
                                f"Failed to save content for doc '{file_name}': {e}"
                            )

                    docs_data.append(
                        {
                            "file_name": file_name,
                            "file_type": file_type,
                            "chunk_strategy": chunk_strategy,
                            "chunk_size": chunk_size,
                            "chunk_overlap": chunk_overlap,
                            "additional_params": additional_params,
                            "content_file": content_file,
                        }
                    )

                except Exception as e:
                    logger.error(
                        f"Failed to process document {doc_idx} in collection "
                        f"'{collection_name}': {e}"
                    )
                    errors.append(f"Failed to process document in '{collection_name}': {e}")
                    continue

            collections_data.append(
                {
                    "collection_name": collection_name,
                    "user_id": user_id,
                    "embedder_id": embedder_id,
                    "agent_ids": agent_ids,
                    "documents": docs_data,
                }
            )
            logger.info(
                f"Processed collection '{collection_name}' with {len(docs_data)} documents"
            )

        except Exception as e:
            logger.error(f"Failed to process collection: {e}")
            errors.append(f"Failed to process collection: {e}")
            continue

    # Get all agents with search config (those who have knowledge_collection assigned)
    try:
        agents_with_knowledge = Agent.objects.filter(
            knowledge_collection_id__isnull=False
        ).values("id", "search_limit", "similarity_threshold")

        for agent_data in agents_with_knowledge:
            similarity = agent_data.get("similarity_threshold")
            agents_search_config.append(
                {
                    "agent_id": agent_data["id"],
                    "search_limit": agent_data.get("search_limit"),
                    "similarity_threshold": str(similarity) if similarity is not None else None,
                }
            )
        logger.info(f"Found {len(agents_search_config)} agents with search configs")

    except Exception as e:
        logger.error(f"Failed to fetch agent search configs: {e}")
        errors.append(f"Failed to fetch agent search configs: {e}")

    # Write collections.json
    collections_path = os.path.join(BACKUP_DIR, "collections.json")
    try:
        with open(collections_path, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "exported_at": datetime.utcnow().isoformat() + "Z",
                    "collections": collections_data,
                },
                f,
                indent=2,
                ensure_ascii=False,
            )
        logger.info(f"Exported {len(collections_data)} collections to {collections_path}")
    except Exception as e:
        logger.error(f"Failed to write collections.json: {e}")
        errors.append(f"Failed to write collections.json: {e}")

    # Write agents_search_config.json
    agents_path = os.path.join(BACKUP_DIR, "agents_search_config.json")
    try:
        with open(agents_path, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "exported_at": datetime.utcnow().isoformat() + "Z",
                    "agents": agents_search_config,
                },
                f,
                indent=2,
                ensure_ascii=False,
            )
        logger.info(f"Exported {len(agents_search_config)} agent configs to {agents_path}")
    except Exception as e:
        logger.error(f"Failed to write agents_search_config.json: {e}")
        errors.append(f"Failed to write agents_search_config.json: {e}")

    # Write errors log if any
    if errors:
        errors_path = os.path.join(BACKUP_DIR, "extraction_errors.log")
        with open(errors_path, "w", encoding="utf-8") as f:
            f.write(f"Extraction errors at {datetime.utcnow().isoformat()}Z\n")
            f.write("=" * 50 + "\n")
            for error in errors:
                f.write(f"- {error}\n")
        logger.warning(
            f"Extraction completed with {len(errors)} errors. See {errors_path}"
        )

    logger.info("Knowledge data extraction completed!")


def reverse_extract(apps, schema_editor):
    """Reverse migration - just log a warning, don't delete backup."""
    logger.warning(
        "Reverse migration: Knowledge backup files are preserved in "
        "data/knowledge_migration_backup/"
    )


class Migration(migrations.Migration):

    dependencies = [
        ("tables", "0129_merge_20251229_1542"),
    ]

    operations = [
        migrations.RunPython(extract_knowledge_data, reverse_extract),
    ]
