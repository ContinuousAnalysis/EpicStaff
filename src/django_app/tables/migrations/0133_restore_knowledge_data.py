# Generated by Django 5.1.3 on 2026-01-19 16:03
# This migration restores data from backup to NEW models after schema change

import json
import os
from decimal import Decimal
from datetime import datetime
from django.db import migrations
from django.conf import settings
from loguru import logger


# Use MEDIA_ROOT (mounted volume) to persist backup across container rebuilds
# Must match the path used in 0130_extract_knowledge_data.py
MEDIA_ROOT = getattr(settings, "MEDIA_ROOT", None) or "/app/media"
BACKUP_DIR = os.path.join(MEDIA_ROOT, "knowledge_migration_backup")
CONTENTS_DIR = os.path.join(BACKUP_DIR, "contents")
COLLECTIONS_FILE = os.path.join(BACKUP_DIR, "collections.json")
AGENTS_CONFIG_FILE = os.path.join(BACKUP_DIR, "agents_search_config.json")

# Prefix to mark migrated collections (for later indexing identification)
MIGRATE_PREFIX = "COLLECTION_MIGRATE_"


def cleanup_backup_files():
    """
    Delete backup files after successful restoration.
    Called only when restore completes without critical errors.
    """
    import shutil

    if not os.path.exists(BACKUP_DIR):
        logger.info("No backup directory to cleanup.")
        return

    try:
        shutil.rmtree(BACKUP_DIR)
        logger.success(f"Cleaned up backup directory: {BACKUP_DIR}")
    except Exception as e:
        logger.warning(f"Failed to cleanup backup directory: {e}")
        logger.warning("You may want to manually delete: " f"{BACKUP_DIR}")


def restore_knowledge_data(apps, schema_editor):
    """
    Restore knowledge data from backup files to NEW models.

    Uses service methods where possible, falls back to direct ORM for edge cases.

    Creates:
    - SourceCollection (with COLLECTION_MIGRATE_ prefix)
    - DocumentContent (direct ORM - service requires UploadedFile)
    - DocumentMetadata
    - BaseRagType + NaiveRag (via NaiveRagService)
    - NaiveRagDocumentConfig (via NaiveRagService)
    - AgentNaiveRag (direct ORM - avoids validation issues)
    - NaiveRagSearchConfig (via SearchConfigService)
    """

    # Check if backup files exist
    if not os.path.exists(COLLECTIONS_FILE):
        logger.info(
            f"No backup file found at {COLLECTIONS_FILE}. "
            "This is expected for fresh installations. Skipping restore."
        )
        return

    try:
        from tables.services.knowledge_services.collection_management_service import (
            CollectionManagementService,
        )
        from tables.services.knowledge_services.naive_rag_service import NaiveRagService
        from tables.services.rag_assignment_service import SearchConfigService
        from tables.models import (
            SourceCollection,
            DocumentMetadata,
            DocumentContent,
            Agent,
        )
        from tables.models.knowledge_models import (
            BaseRagType,
            NaiveRag,
            NaiveRagDocumentConfig,
            AgentNaiveRag,
        )
        from tables.models.embedding_models import EmbeddingConfig
    except ImportError as e:
        logger.error(f"Failed to import required modules: {e}")
        logger.warning("Skipping restore - models or services not available.")
        return

    errors = []
    stats = {
        "collections_created": 0,
        "documents_created": 0,
        "naive_rags_created": 0,
        "document_configs_created": 0,
        "agent_links_created": 0,
        "search_configs_created": 0,
    }

    # Load collections data
    try:
        with open(COLLECTIONS_FILE, "r", encoding="utf-8") as f:
            collections_data = json.load(f)
        logger.info(
            f"Loaded backup from {COLLECTIONS_FILE} "
            f"(exported at: {collections_data.get('exported_at', 'unknown')})"
        )
    except Exception as e:
        logger.error(f"Failed to read collections.json: {e}")
        return

    collections_list = collections_data.get("collections", [])
    logger.info(f"Found {len(collections_list)} collections to restore")

    # Process each collection
    for col_idx, col_data in enumerate(collections_list):
        try:
            collection_name = col_data.get(
                "collection_name", f"Unnamed Collection {col_idx}"
            )
            user_id = col_data.get("user_id", "dummy_user")
            embedder_id = col_data.get("embedder_id")
            agent_ids = col_data.get("agent_ids", [])
            documents = col_data.get("documents", [])

            prefixed_name = f"{MIGRATE_PREFIX}{collection_name}"

            # Check if collection with this name already exists
            existing = SourceCollection.objects.filter(
                user_id=user_id, collection_name=prefixed_name
            ).first()
            if existing:
                logger.warning(
                    f"Collection '{prefixed_name}' already exists (id={existing.collection_id}). Skipping."
                )
                continue

            # 1. Create SourceCollection using service
            try:
                collection = CollectionManagementService.create_collection(
                    collection_name=prefixed_name, user_id=user_id
                )
                stats["collections_created"] += 1
                logger.info(
                    f"Created collection '{prefixed_name}' (id={collection.collection_id})"
                )
            except Exception as e:
                logger.error(f"Failed to create collection '{prefixed_name}': {e}")
                errors.append(f"Failed to create collection '{collection_name}': {e}")
                continue

            # 2. Create documents (direct ORM - service requires UploadedFile)
            # Store mapping of file_name -> document for config updates
            doc_mapping = {}

            for doc_idx, doc_data in enumerate(documents):
                try:
                    file_name = doc_data.get("file_name", f"unnamed_doc_{doc_idx}")
                    file_type = doc_data.get("file_type", "")
                    content_file = doc_data.get("content_file")

                    # Load binary content if available
                    document_content = None
                    file_size = None

                    if content_file:
                        content_path = os.path.join(CONTENTS_DIR, content_file)
                        if os.path.exists(content_path):
                            try:
                                with open(content_path, "rb") as f:
                                    binary_content = f.read()
                                file_size = len(binary_content)

                                # Create DocumentContent directly (service requires UploadedFile)
                                document_content = DocumentContent.objects.create(
                                    content=binary_content
                                )
                                logger.debug(
                                    f"Created DocumentContent for '{file_name}' ({file_size} bytes)"
                                )
                            except Exception as e:
                                logger.error(
                                    f"Failed to read content file '{content_file}': {e}"
                                )
                                errors.append(
                                    f"Failed to read content for '{file_name}': {e}"
                                )
                        else:
                            logger.warning(f"Content file not found: {content_path}")

                    # Create DocumentMetadata
                    document = DocumentMetadata.objects.create(
                        source_collection=collection,
                        document_content=document_content,
                        file_name=file_name,
                        file_type=file_type,
                        file_size=file_size,
                    )
                    stats["documents_created"] += 1

                    # Store mapping for later config updates
                    doc_mapping[file_name] = {
                        "document": document,
                        "chunk_strategy": doc_data.get("chunk_strategy", "token"),
                        "chunk_size": min(
                            8000, max(20, doc_data.get("chunk_size", 1000))
                        ),
                        "chunk_overlap": doc_data.get("chunk_overlap", 150),
                        "additional_params": doc_data.get("additional_params", {}),
                    }

                    logger.debug(
                        f"Created DocumentMetadata: '{file_name}' (id={document.document_id})"
                    )

                except Exception as e:
                    logger.error(
                        f"Failed to create document '{doc_data.get('file_name', 'unknown')}': {e}"
                    )
                    errors.append(f"Failed to create document: {e}")
                    continue

            # Update collection status if documents were added
            if documents:
                collection.status = SourceCollection.SourceCollectionStatus.UPLOADING
                collection.save(update_fields=["status", "updated_at"])

            # 3. Create NaiveRag (if we have documents)
            naive_rag = None
            if doc_mapping:
                try:
                    # Validate embedder exists
                    embedder_exists = False
                    if embedder_id:
                        embedder_exists = EmbeddingConfig.objects.filter(
                            pk=embedder_id
                        ).exists()
                        if not embedder_exists:
                            logger.warning(
                                f"EmbeddingConfig with id={embedder_id} not found for collection "
                                f"'{collection_name}'. Creating NaiveRag with embedder=None."
                            )

                    if embedder_id and embedder_exists:
                        naive_rag = NaiveRagService.create_or_update_naive_rag(
                            collection_id=collection.collection_id,
                            embedder_id=embedder_id,
                        )
                    else:
                        # Create NaiveRag without embedder
                        base_rag_type = BaseRagType.objects.create(
                            source_collection=collection,
                            rag_type=BaseRagType.RagType.NAIVE,
                        )
                        naive_rag = NaiveRag.objects.create(
                            base_rag_type=base_rag_type,
                            embedder=None,
                            rag_status=NaiveRag.NaiveRagStatus.NEW,
                        )
                        logger.warning(
                            f"Created NaiveRag {naive_rag.naive_rag_id} without embedder. "
                            "You will need to set an embedder before indexing!!!"
                        )

                    stats["naive_rags_created"] += 1
                    logger.info(f"Created NaiveRag (id={naive_rag.naive_rag_id})")

                except Exception as e:
                    logger.error(
                        f"Failed to create NaiveRag for collection '{collection_name}': {e}"
                    )
                    errors.append(f"Failed to create NaiveRag: {e}")
                    naive_rag = None

            # 4. Initialize document configs and update with old params
            if naive_rag:
                try:
                    # Initialize configs with defaults using service
                    NaiveRagService.init_document_configs(naive_rag.naive_rag_id)
                    logger.debug(
                        f"Initialized document configs for NaiveRag {naive_rag.naive_rag_id}"
                    )

                    # Update each config with OLD chunk params
                    configs = NaiveRagDocumentConfig.objects.filter(
                        naive_rag=naive_rag
                    ).select_related("document")

                    for config in configs:
                        doc_info = doc_mapping.get(config.document.file_name)
                        if not doc_info:
                            continue

                        strategies = [doc_info["chunk_strategy"], "character"]

                        for strategy in strategies:
                            try:
                                NaiveRagService.update_document_config(
                                    config_id=config.naive_rag_document_id,
                                    naive_rag_id=naive_rag.naive_rag_id,
                                    chunk_size=doc_info["chunk_size"],
                                    chunk_overlap=doc_info["chunk_overlap"],
                                    chunk_strategy=strategy,
                                    additional_params=doc_info["additional_params"],
                                )
                                stats["document_configs_created"] += 1
                                logger.debug(
                                    f"Updated config for '{config.document.file_name}' "
                                    f"with chunk_size={doc_info['chunk_size']}, "
                                    f"chunk_overlap={doc_info['chunk_overlap']}, "
                                    f"chunk_strategy={strategy}"
                                )
                                break  # Success, exit loop
                            except Exception as e:
                                if strategy == "character":
                                    logger.error(
                                        f"Failed to update config for '{config.document.file_name}': {e}"
                                    )
                                    errors.append(f"Failed to update doc config: {e}")
                                else:
                                    logger.error(
                                        f"Failed to update config for '{config.document.file_name}', trying 'character' strategy: {e}"
                                    )

                except Exception as e:
                    logger.error(f"Failed to initialize document configs: {e}")
                    errors.append(f"Failed to init document configs: {e}")

            # 5. Link agents to collection and NaiveRag
            for agent_id in agent_ids:
                try:
                    agent = Agent.objects.filter(pk=agent_id).first()
                    if not agent:
                        logger.warning(
                            f"Agent with id={agent_id} not found. Skipping link."
                        )
                        continue

                    # Set agent's knowledge_collection
                    agent.knowledge_collection = collection
                    agent.save(update_fields=["knowledge_collection"])

                    # Create AgentNaiveRag link (direct ORM to avoid validation issues)
                    if naive_rag:
                        # Check if link already exists
                        existing_link = AgentNaiveRag.objects.filter(
                            agent=agent
                        ).first()
                        if existing_link:
                            logger.warning(
                                f"Agent {agent_id} already has a NaiveRag link. Skipping."
                            )
                            continue

                        AgentNaiveRag.objects.create(agent=agent, naive_rag=naive_rag)
                        stats["agent_links_created"] += 1
                        logger.info(
                            f"Linked Agent {agent_id} to collection {collection.collection_id} "
                            f"and NaiveRag {naive_rag.naive_rag_id}"
                        )
                    else:
                        logger.info(
                            f"Linked Agent {agent_id} to collection {collection.collection_id} "
                            "(no NaiveRag available)"
                        )

                except Exception as e:
                    logger.error(f"Failed to link agent {agent_id}: {e}")
                    errors.append(f"Failed to link agent {agent_id}: {e}")
                    continue

            logger.info(
                f"Completed collection '{collection_name}': "
                f"{len(documents)} documents, {len(agent_ids)} agent links"
            )

        except Exception as e:
            logger.error(
                f"Failed to process collection '{col_data.get('collection_name', 'unknown')}': {e}"
            )
            errors.append(f"Failed to process collection: {e}")
            continue

    # 6. Restore search configs from agents_search_config.json
    if os.path.exists(AGENTS_CONFIG_FILE):
        try:
            with open(AGENTS_CONFIG_FILE, "r", encoding="utf-8") as f:
                agents_config_data = json.load(f)

            agents_list = agents_config_data.get("agents", [])
            logger.info(f"Found {len(agents_list)} agent search configs to restore")

            for agent_config in agents_list:
                try:
                    agent_id = agent_config.get("agent_id")
                    search_limit = agent_config.get("search_limit")
                    similarity_threshold = agent_config.get("similarity_threshold")

                    if agent_id is None:
                        continue

                    agent = Agent.objects.filter(pk=agent_id).first()
                    if not agent:
                        logger.warning(
                            f"Agent with id={agent_id} not found. Skipping search config."
                        )
                        continue

                    if similarity_threshold is not None:
                        similarity_threshold = float(similarity_threshold)

                    # Use service to create/update search config
                    SearchConfigService.update_search_config(
                        agent=agent,
                        search_limit=search_limit,
                        similarity_threshold=similarity_threshold,
                    )
                    stats["search_configs_created"] += 1
                    logger.debug(
                        f"Created/updated search config for agent {agent_id}: "
                        f"search_limit={search_limit}, similarity_threshold={similarity_threshold}"
                    )

                except Exception as e:
                    logger.error(
                        f"Failed to create search config for agent {agent_config.get('agent_id')}: {e}"
                    )
                    errors.append(f"Failed to create search config: {e}")
                    continue

        except Exception as e:
            logger.error(f"Failed to read agents_search_config.json: {e}")
            errors.append(f"Failed to read agents config: {e}")

    # Write errors log if any
    if errors:
        errors_path = os.path.join(BACKUP_DIR, "restoration_errors.log")
        try:
            with open(errors_path, "w", encoding="utf-8") as f:
                f.write(f"Restoration errors at {datetime.utcnow().isoformat()}Z\n")
                f.write("=" * 50 + "\n")
                for error in errors:
                    f.write(f"- {error}\n")
            logger.warning(
                f"Restoration completed with {len(errors)} errors. See {errors_path}"
            )
        except Exception as e:
            logger.error(f"Failed to write errors log: {e}")

    # Log summary
    logger.info("=" * 60)
    logger.info("Knowledge data restoration completed!")
    logger.info(f"  Collections created: {stats['collections_created']}")
    logger.info(f"  Documents created: {stats['documents_created']}")
    logger.info(f"  NaiveRags created: {stats['naive_rags_created']}")
    logger.info(f"  Document configs: {stats['document_configs_created']}")
    logger.info(f"  Agent links created: {stats['agent_links_created']}")
    logger.info(f"  Search configs created: {stats['search_configs_created']}")
    logger.info(f"  Errors: {len(errors)}")
    logger.info("=" * 60)
    logger.info(f"Collections with prefix '{MIGRATE_PREFIX}' need to be re-indexed.")
    logger.success("Run the indexing for each collection after all services are ready.")
    logger.success(
        "Command: [docker exec -it django_app python manage.py index_migrated_collections --dry-run] - for preview."
        "[docker exec -it django_app python manage.py index_migrated_collections] - execute"
    )
    logger.info("=" * 60)

    # Cleanup backup files after restoration completes
    # Cleanup regardless of whether data was restored (handles fresh installs with stale backup)
    cleanup_backup_files()


def reverse_restore(apps, schema_editor):
    """
    Reverse migration - delete restored data.
    Deletes all collections with the MIGRATE prefix.
    """
    try:
        from tables.models import SourceCollection
    except ImportError:
        logger.warning("Could not import SourceCollection for reverse migration.")
        return

    migrated = SourceCollection.objects.filter(
        collection_name__startswith=MIGRATE_PREFIX
    )
    count = migrated.count()

    if count > 0:
        logger.warning(
            f"Deleting {count} migrated collections (with '{MIGRATE_PREFIX}' prefix)"
        )
        migrated.delete()
        logger.info("Reverse migration completed. Migrated collections deleted.")
    else:
        logger.info("No migrated collections found to delete.")


class Migration(migrations.Migration):
    """
    Restores knowledge data from backup files to NEW models.

    After this migration:
    - Collections are created with 'COLLECTION_MIGRATE_' prefix
    - NaiveRags are created (some may have embedder=None)
    - Document configs have old chunking parameters restored
    - Agents are linked to collections and NaiveRags
    - Search configs (search_limit, similarity_threshold) are restored

    NEXT STEPS:
    - Run indexing for each NaiveRag to generate chunks and embeddings
    - Remove 'COLLECTION_MIGRATE_' prefix from collection names after successful indexing
    """

    dependencies = [
        ("tables", "0132_knowledge_refactor"),
    ]

    operations = [
        migrations.RunPython(restore_knowledge_data, reverse_restore),
    ]
